{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Introduction\n",
    "\n",
    "[Dask](https://dask.org/) is a Python library suited for block-wise parallelization of most Numpy and Pandas tasks. It is also useful for performing operations on out-of-memory data by automating the processing of paging chunks to and from storage. While there is a wonderful [tutorial](https://github.com/dask/dask-tutorial) available on GitHub, it does not show off some of the more interesting capabilities of Dask. The purpose of this notebook is to demonstrate how to use the `map_overlap` function to downscale an out-of-memory volumetric (3D) image. That being said, this notebook will make more sense if you go through the tutorial first.\n",
    "\n",
    "### More Details\n",
    "\n",
    "The way Dask operates is by creating a lazily-evaluated, directed acyclic graph of all the tasks its needs to perform to carry out its instructions, and assigning tasks in the correct order to workers in a distributed environment. What that means is that Dask doesn't do any computation until data is actually needed, or instructed directly by the `.compute()` method. The acyclic directed task graph part means that Dask figures out the order of operations necessary to perform what you've asked it to do and what operations depend on what other operations, and sets them up following the logic of the code you've written. Acyclic means that there can't be any cycles, which means no while loops and no recursion. When Dask is explicitly (via `.compute()`) or implicitly (via `.imshow()`, etc.) instructed to perform computations, the workers are assigned tasks from the task graph in the order indicated.\n",
    "\n",
    "It is possible to visualize the task graph using `.visualize()`. The method is not used in this notebook because the task graphs are large and difficult to see in the drawing. The tutorial referenced above is a good source for more information. Progress of the tasks may be visualized on the task graph using the real-time dashboard, which will be explained closer to where it is set up.\n",
    "\n",
    "## Setup\n",
    "\n",
    "To use this notebook, you will need the following packages:\n",
    " - dask (>=2.5.2)\n",
    " - distributed (>=2.5.2)\n",
    " - numpy (>=1.16.5)\n",
    " - scikit-image (>=0.15.0)\n",
    "\n",
    "Assuming you are using conda as your package manager (recommended!), cloning a new base enviroment using `conda create --name <name> --clone base`, activating the environment using either `conda activate <name>` or `source activate <name>` and running `conda install dask` should be sufficient.\n",
    "\n",
    "### Library Imports and Helper Functions\n",
    "\n",
    "Here we import packages and set up a couple of useful helper functions for downstream computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mem_mb(shape):\n",
    "    '''Determines memory consumption of an array with shape in MB'''\n",
    "    from functools import reduce\n",
    "    from operator import mul\n",
    "    count = reduce(mul, shape, 1)\n",
    "    return count * 8 / ( 1024 ** 2 )\n",
    "\n",
    "def display(image):\n",
    "    '''Shows an image in the Jupyter notebook.'''\n",
    "    from skimage import io\n",
    "    io.imshow(image)\n",
    "    io.show()\n",
    "\n",
    "def throughput(mem_mb, time_s):\n",
    "    print(\"MB/s: {:.2f}\".format(mem_mb / time_s))\n",
    "\n",
    "def stats_da(image):\n",
    "    print(\"Min: {min:.2f}\\nMax: {max:.2f}\\nMean: {mean:.2f}\\nStd: {std:.2f}\".format(\n",
    "        max=image.max().compute(),\n",
    "        min=image.min().compute(),\n",
    "        mean=image.mean().compute(),\n",
    "        std=image.std().compute()))\n",
    "\n",
    "def stats_np(image):\n",
    "    print(\"Min: {min:.2f}\\nMax: {max:.2f}\\nMean: {mean:.2f}\\nStd: {std:.2f}\".format(\n",
    "        max=image.max(),\n",
    "        min=image.min(),\n",
    "        mean=image.mean(),\n",
    "        std=image.std()))\n",
    "\n",
    "def scale_shape(shape, factor):\n",
    "    '''Gives the expected shape if scaled by factor'''\n",
    "    return tuple(map(lambda x: round(x * factor), shape))\n",
    "\n",
    "def rescale(chunk, factor, order=0):\n",
    "    '''Rescales a chunk based on scaling factor.'''\n",
    "    from skimage.transform import rescale\n",
    "    return rescale(chunk, factor, order=order, multichannel=False)\n",
    "\n",
    "def mean_downscale(chunk, factors):\n",
    "    '''Resizes a chunk using the integer-factor local-mean downscaling from scikit-image'''\n",
    "    from skimage.transform import downscale_local_mean\n",
    "    return downscale_local_mean(chunk, factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Distibuted Client and Dashboard Setup\n",
    "\n",
    "As mentioned earler, Dask comes with a helpful real-time dashboard for understanding how tasks are distributed to workers. The dashboard is useful for developing intuition about parallel code performance. More information is available in the Dask tutorial, linked in the introduction at the top of the notebook. Below we set up the Dask distributed client and get a URL for the dashboard.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "c = Client()\n",
    "port = c.scheduler_info()['services']['dashboard']\n",
    "print(\"Type `http://localhost:{port}` into the URL bar of your favorite browser to watch the following code in action on your machine in real time.\".format(port=port))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computation\n",
    "\n",
    "### Preparation\n",
    "\n",
    "Here we prepare some basic constants for downstream computation, like volumetric image size and chunk size. If at any point you receive any error messages about running out of memory on workers, reduce the chunk size. Feel free to play around with these values and re-run the notebook. It is recommended to follow along with the dashboard in your browser, see above for more information. Keep in mind that chunks can't be larger than the shape. Chunks that are too small will create significant overhead in the task graph and require more communication between workers, reducing efficiency. Chunks that are too large may consume too much memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = 3\n",
    "size_len = 400\n",
    "size = ndim * [size_len]\n",
    "mem_mb = compute_mem_mb(size)\n",
    "print(\"3D array size (MB): {:.2f}\".format(mem_mb))\n",
    "chunk_len = 100\n",
    "chunks = ndim * [chunk_len]\n",
    "chunk_mem_mb = compute_mem_mb(chunks)\n",
    "print(\"Chunk size (MB): {:.2f}\".format(chunk_mem_mb))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we create a vaguely interesting array of values which increase radially from the origin. To do this, we use the `da.indices()` function to create a collection of Dask arrays whose values are equal to their index. One array is created per dimension, giving us three arrays in the volumetric case.\n",
    "\n",
    "We then stack the three arrays using `da.stack()` along the fourth dimension (`axis=3`) to create a 3D array of coordinate vectors.\n",
    "\n",
    "To create the radial image we square every element, sum along the fourth dimension to add up the squares along the coordinate dimension (`axis=3` again), and then find the square root of every element.\n",
    "\n",
    "Finally we normalize the array values by the maximum value over the entire array.\n",
    "\n",
    "The construct `t = %timeit -n1 -r1 -o -q fn()` is IPython magic which means:\n",
    " - `%timeit`: time `fn()` on this line only...\n",
    " - `-n1`: looping one time...\n",
    " - `-r1`: repeating loops one time...\n",
    " - `-o`: output a value (and assign to `t`)...\n",
    " - `-q`: silently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = da.indices(size, dtype='float', chunks=chunks)\n",
    "grid = da.stack(coords, axis=3)\n",
    "base_image = (grid ** 2).sum(axis=3) ** 0.5\n",
    "image = base_image / base_image.max()\n",
    "t = %timeit -n1 -r1 -o -q display(image[(None, 0)].squeeze())\n",
    "throughput(mem_mb, t.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Here we generate some image statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = %timeit -n1 -r1 -o -q stats_da(image)\n",
    "throughput(mem_mb, t.average)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Image Rescaling\n",
    "\n",
    "It is worth noting that any naive block-based image rescaling is likely to create artifacts somewhere in the image, typically at the image boundary or at the interface between blocks. Unfortunately a careful, distributed implementation of rescaling is not available at the time of writing, so we have to use our own naive implementation.\n",
    "\n",
    "Here we perform image rescaling using the local-mean downscaling method available in `scikit-image`. This downscaling method uses an integer scaling `factor`, and returns the mean of values in a cube-shaped window whose side length is equal to factor. The window slides across intervals equal to `factor`. With the above note in mind, we should be careful that the chunk size is divisible by the integer scaling factor to ensure proper integer scaling.  by having the image size divisible by the scaling factor, but this is not always possible in practice.\n",
    "\n",
    "An important detail is that we can't use `map_overlaps()` directly because the function being mapped (`mean_downscale`) changes the chunk size. The `map_overlaps()` method assumes the same depth for both the overlapping and trimming operations, which is not correct for rescaling. For our problem, an overlap equal to `factor` is reduced to a trim of 1.\n",
    "\n",
    "Note that a larger factor reduces throughput. This is because the `overlap` functionality creates new chunks from the originals padded by values in neighboring chunks up to the supplied depth. If the chunk is at the image boundary, the supplied boundary is used there. In our case, a reflected boundary is used. In this particular problem, the issue doesn't become apparent until a large scaling factor is used. Try setting `factor` equal to the chunk length. On my machine the computation took about 5 times as long as it did for values equal to or below 20. The reason for this is the increase in inter-worker communication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_factor = 5\n",
    "window_factors = ndim * tuple([window_factor])\n",
    "boundary = 'reflect'\n",
    "\n",
    "overlapped = da.overlap.overlap(image, depth=window_factor, boundary=boundary)\n",
    "\n",
    "new_chunks = tuple(map(lambda x: round(x / window_factor), chunks))\n",
    "mapped = overlapped.map_blocks(lambda x: mean_downscale(x, window_factors), chunks=new_chunks, dtype=image.dtype)\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "local_mean_scaled_image = da.overlap.trim_overlap(mapped, depth=1).compute()\n",
    "\n",
    "end = time.time()\n",
    "t = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput(mem_mb, t)\n",
    "display(local_mean_scaled_image[(None, 0)].squeeze())\n",
    "stats_np(local_mean_scaled_image)\n",
    "print(\"Output shape: {}\".format(local_mean_scaled_image.shape))\n",
    "print(\"Expected shape: {}\".format(scale_shape(image.shape, 1 / window_factor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You may have noticed that because the chunks are divisble by the scaling factor, we don't need overlapping at all! To explain, consider that the overlap is equal to the factor. Because the window slides by a distance equal to factor and the overlap depth is equal to factor, the window never combines overlapped values with chunk values. Given this fact, we can rewrite the previous code more simply as below using only a call to `map_blocks()` without calls to `overlaps`. Note the maximum error is exactly zero, indicating no difference in code output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "local_mean_scaled_image_s = image.map_blocks(lambda x: mean_downscale(x, window_factors), dtype=image.dtype).compute()\n",
    "\n",
    "end = time.time()\n",
    "t = end - start\n",
    "throughput(mem_mb, t)\n",
    "\n",
    "error = np.abs(local_mean_scaled_image - local_mean_scaled_image_s)\n",
    "display(error[(None, 0)].squeeze())\n",
    "\n",
    "max_error = error.max()\n",
    "identical = max_error == 0.0\n",
    "print(\"Maximum error: {:e}\".format(max_error))\n",
    "print(\"Identically zero: {}\".format(identical))\n",
    "\n",
    "print(\"Output shape: {}\".format(local_mean_scaled_image_s.shape))\n",
    "print(\"Expected shape: {}\".format(scale_shape(image.shape, 1 / window_factor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "What happens if we replace `window_factor` with a value that doesn't evenly divide `chunk_len`? Try setting `window_factor = 7` in the previous cells and rerunning them. Notice anything off with the previous error image? There should be a grid-patterned artifact. The reason for the artifacting is because `window_factor` doesn't evenly divide `chunk_len`. Recall that the window slides by a value equal to `window_factor`, which means the windows at the ends of each chunk axis lie partly outside the chunk. Dask appears to use the constant value 0 for elements outside the chunk. As a result, the value of the mean is reduced in windows which lie partly out of a chunk, causing the grid artifact.\n",
    "\n",
    "### Spline Interpolation Rescaling\n",
    "\n",
    "Here we perform cubic spline interpolation scaling on the image. The value of `factor` plays a different role here. Rather than being an integer window size, it represents the factor to multiply the shape of the image. We are intentionally using a factor value which does not produce an even multiple of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1 / 3\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "cubic_scaled_image = image.map_blocks(lambda x: rescale(x, factor, order=3), dtype=image.dtype).compute()\n",
    "\n",
    "end = time.time()\n",
    "t = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "throughput(mem_mb, t)\n",
    "display(cubic_scaled_image[(None, 0)].squeeze())\n",
    "stats_np(cubic_scaled_image)\n",
    "print(\"Output shape: {}\".format(cubic_scaled_image.shape))\n",
    "print(\"Expected shape: {}\".format(scale_shape(image.shape, factor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validation\n",
    "\n",
    "To be sure the results align with expectation, we can rescale a Numpy array of a slice together with a Dask `map_blocks()` rescaling of the same slice, and then compare them. For this sort of validation to work we need a scaling factor that results in an integer shape to ensure the shape is the same for both rescaled images.\n",
    "\n",
    "As we can see below, the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor = 1 / 4\n",
    "\n",
    "image_slice_da = image[(None, 0)].squeeze()\n",
    "cubic_slice_da = np.asarray(image_slice_da.map_blocks(lambda x: rescale(x, factor, order=3), dtype=image_slice_da.dtype).compute())\n",
    "\n",
    "image_slice_np = np.asarray(image_slice_da.compute())\n",
    "cubic_slice_np = rescale(image_slice_np, factor, order=3)\n",
    "\n",
    "error_slice = np.abs(cubic_slice_da - cubic_slice_np)\n",
    "max_error_slice = error_slice.max()\n",
    "display(error[(None, 0)].squeeze())\n",
    "\n",
    "identical = max_error == 0.0\n",
    "print(\"Maximum error: {:e}\".format(max_error))\n",
    "print(\"Identically zero: {}\".format(identical))\n",
    "\n",
    "print(\"Output shape: {}\".format(local_mean_scaled_image_s.shape))\n",
    "print(\"Expected shape: {}\".format(scale_shape(image.shape, 1 / window_factor)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Concluding Remarks\n",
    "\n",
    "We hope you found this brief notebook useful for understanding how Dask can be applied to certain image analysis problems using `overlap` and `map_blocks`. If you need more information, please refer to the Dask website linked in the introduction. If you liked this notebook, feel free to contact the author."
   ]
  }
 ]
}